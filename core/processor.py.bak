import os
import json
import pandas as pd
import logging

from typing import Dict, List, Any, Optional, cast
from decimal import Decimal

# Logger local para uso interno do módulo (não configura nível globalmente)
logger = logging.getLogger(__name__)


class ECDProcessor:
    """
    Motor de Processamento de Dados ECD (SPED-Contábil) com Auditoria Integrada.
    """

    def __init__(
        self,
        registros: List[Dict[str, Any]],
        cnpj: str = "",
        layout_versao: str = "",
        knowledge_base: Optional[Any] = None,
    ):
        self.df_bruto = pd.DataFrame(registros) if registros else pd.DataFrame()
        self.cnpj = cnpj
        self.layout_versao = layout_versao
        self.knowledge_base = knowledge_base
        self.blocos: Dict[str, pd.DataFrame] = {}
        self.cod_plan_ref: Optional[str] = None
        self.ano_vigencia: Optional[int] = None

        # Path para o catálogo de planos referenciais
        self.catalog_path = os.path.normpath(
            os.path.join(
                os.path.dirname(__file__),
                "..",
                "schemas",
                "ref_plans",
                "ref_catalog.json",
            )
        )

        if not self.df_bruto.empty:
            self._separar_blocos()
            self._identificar_metadados_referenciais()

    def _obter_arquivos_referenciais(self) -> List[str]:
        """
        Localiza todos os arquivos CSV (Balanço, DRE, etc) no ref_catalog.json
        para a instituição e ano vigentes.
        """
        if not self.cod_plan_ref or not self.ano_vigencia:
            return []

        if not os.path.exists(self.catalog_path):
            logger.error(f"Catálogo não encontrado: {self.catalog_path}")
            return []

        try:
            with open(self.catalog_path, "r", encoding="utf-8") as f:
                catalog = json.load(f)

            # 1. Filtro Instituição
            inst = catalog.get(str(self.cod_plan_ref))
            if not inst:
                return []

            # 2. Filtro Vigência (Range)
            período_escolhido = None
            períodos_disponíveis = []

            for key, info in inst.items():
                r_min, r_max = info.get("range", [0, 0])
                períodos_disponíveis.append((r_min, r_max, info))
                if r_min <= self.ano_vigencia <= r_max:
                    período_escolhido = info
                    break

            # Se não achou vigência exata, busca o período mais próximo (abordagem cross-temporal)
            if not período_escolhido and períodos_disponíveis:
                logger.info(
                    f"Ano {self.ano_vigencia} não mapeado para plano {self.cod_plan_ref}. "
                    "Buscando período referencial compatível..."
                )
                períodos_disponíveis.sort(key=lambda x: abs(x[1] - self.ano_vigencia))
                período_escolhido = períodos_disponíveis[0][2]

            if not período_escolhido:
                return []

            # 3. Coleta todos os arquivos de planos disponíveis para o período
            arquivos = []
            plans = período_escolhido.get("plans", {})
            for alias in plans:
                # Pega a maior versão disponível para cada alias (L100, L300, etc)
                versões = sorted(
                    plans[alias].keys(), key=lambda v: int(v), reverse=True
                )
                if versões:
                    v_top = versões[0]
                    filename = plans[alias][v_top].get("file")
                    if filename:
                        caminho = os.path.normpath(
                            os.path.join(
                                os.path.dirname(self.catalog_path), "data", filename
                            )
                        )
                        if os.path.exists(caminho):
                            arquivos.append(caminho)

            return arquivos

        except Exception as e:
            logger.error(f"Erro ao consultar catálogo: {e}")
            return []

    def _separar_blocos(self) -> None:
        """Divide os registros por REG e limpa prefixos redundantes."""
        versoes_reg = cast(pd.Series, self.df_bruto["REG"]).unique()
        for reg in versoes_reg:
            df_reg = (
                self.df_bruto[self.df_bruto["REG"] == reg]
                .dropna(axis=1, how="all")
                .copy()
            )
            if "REG" in df_reg.columns:
                df_reg = df_reg.drop(columns=["REG"])

            prefixo = f"{reg}_"
            df_reg.columns = pd.Index(
                [
                    str(c).replace(prefixo, "") if str(c).startswith(prefixo) else c
                    for c in df_reg.columns
                ]
            )

            # Remove duplicatas de colunas que possam surgir na renomeação
            self.blocos[f"dfECD_{reg}"] = df_reg.loc[
                :, ~pd.Index(df_reg.columns).duplicated()
            ].copy()

    def _identificar_metadados_referenciais(self) -> None:
        """Determina o Ano e o Código da Instituição (Funil de Metadados)."""
        df_0000 = self.blocos.get("dfECD_0000")
        if df_0000 is None or df_0000.empty:
            return

        # 1. Identificação do Ano (DT_FIN) - Comum a todas as versões
        val_0000 = df_0000.iloc[0]
        dt_fin = val_0000.get("DT_FIN")
        if hasattr(dt_fin, "year") and dt_fin is not None:
            self.ano_vigencia = int(getattr(dt_fin, "year"))
        elif isinstance(dt_fin, str) and len(dt_fin) >= 8:
            # Tenta DDMMYYYY ou YYYYMMDD
            try:
                self.ano_vigencia = (
                    int(dt_fin[:4]) if int(dt_fin[:4]) > 1900 else int(dt_fin[-4:])
                )
            except (ValueError, IndexError):
                pass

        # 2. Identificação do COD_PLAN_REF (Condicional por Versão)
        try:
            versao_num = float(str(self.layout_versao)) if self.layout_versao else 0.0
        except ValueError:
            versao_num = 0.0

        if versao_num >= 8.0:
            # Moderno: Está no 0000
            self.cod_plan_ref = str(df_0000.iloc[0].get("COD_PLAN_REF", ""))
        else:
            # Legado: Está no primeiro I051
            df_i051 = self.blocos.get("dfECD_I051")
            if df_i051 is not None and not df_i051.empty:
                self.cod_plan_ref = str(df_i051.iloc[0].get("COD_PLAN_REF", ""))

        if not self.cod_plan_ref:
            # --- NÍVEL 1.5: Inferência de Instituição ---
            if self.knowledge_base is not None:
                inferred = self.knowledge_base.get_inferred_plan(
                    self.cnpj, ano_alvo=str(self.ano_vigencia)
                )
                if inferred:
                    self.cod_plan_ref = str(inferred)
                    logger.info(
                        f"COD_PLAN_REF inferido via histórico: {self.cod_plan_ref}"
                    )

        if not self.cod_plan_ref:
            logger.warning(
                f"COD_PLAN_REF não localizado (Versão: {self.layout_versao}). "
                "O mapeamento RFB pode falhar."
            )

    def _converter_decimal(self, valor: Any) -> Decimal:
        """Garante precisão absoluta para cálculos financeiros."""
        if pd.isna(valor) or str(valor).strip() == "" or valor is None:
            return Decimal("0.00")
        try:
            return Decimal(str(valor))
        except Exception:
            return Decimal("0.00")

    def processar_plano_contas(self) -> pd.DataFrame:
        """Processa o Plano de Contas da Empresa (I050) integrado com o Referencial (I051)."""
        df_i050 = self.blocos.get("dfECD_I050")
        df_i051 = self.blocos.get("dfECD_I051")

        if df_i050 is None:
            return pd.DataFrame()

        # Seleciona colunas básicas do I050 e NORMALIZA (remove prefixos se houver)
        map_cols = {c: c.replace("I050_", "") for c in df_i050.columns}
        df_res = df_i050.rename(columns=map_cols).copy()

        cols_essenciais = [
            "PK",
            "COD_NAT",
            "IND_CTA",
            "NIVEL",
            "COD_CTA",
            "COD_CTA_SUP",
            "CTA",
        ]
        df_res = df_res[[c for c in cols_essenciais if c in df_res.columns]]

        # Integração com I051 (Mapeamento Referencial)
        if df_i051 is not None and not df_i051.empty:
            # Normaliza colunas do I051 também
            df_ref = df_i051.rename(columns=lambda x: x.replace("I051_", "")).copy()
            df_ref = df_ref[["FK_PAI", "COD_CTA_REF"]]

            # Left join para garantir que não perdemos contas sintéticas do I050
            df_res = pd.merge(
                df_res, df_ref, left_on="PK", right_on="FK_PAI", how="left"
            )
            df_res.drop(columns=["FK_PAI"], inplace=True, errors="ignore")
        else:
            df_res["COD_CTA_REF"] = None

        df_res["CNPJ"] = self.cnpj

        # --- NÍVEL 1.5: Aprendizado Cross-Temporal Bidirecional ---
        # Se ainda houver contas sem mapeamento e tivermos uma base de conhecimento, tentamos inferir.
        if self.knowledge_base is not None:
            mask_vazio = cast(pd.Series, df_res["COD_CTA_REF"]).isna() | (
                cast(pd.Series, df_res["COD_CTA_REF"]) == ""
            )
            # Foca apenas em contas ANALÍTICAS, conforme sua sugestão
            mask_analitica = df_res["IND_CTA"].astype(str).str.upper() == "A"
            mask_alvo = mask_vazio & mask_analitica

            if mask_alvo.any():
                ano_str = str(self.ano_vigencia) if self.ano_vigencia else ""

                def inferir_ref(row: pd.Series) -> pd.Series:
                    cod_cta = str(row.get("COD_CTA", ""))
                    cod_sup = str(row.get("COD_CTA_SUP", ""))
                    if not cod_cta:
                        return pd.Series([None, "SEM_COD_CTA"])

                    mapper = cast(Any, self.knowledge_base)
                    # Chamada com suporte a COD_SUP para Rodada 2
                    vinculo = mapper.get_mapping(
                        self.cnpj, cod_cta, ano_str, cod_sup=cod_sup
                    )
                    return pd.Series(
                        [vinculo.get("COD_CTA_REF"), vinculo.get("ORIGEM_MAP")]
                    )

                # Aplicamos a busca no "vizinho mais similar" (mapeamento cross-temporal analítico)
                novos_dados = df_res.loc[mask_alvo].apply(inferir_ref, axis=1)
                if not novos_dados.empty:
                    df_res.loc[mask_alvo, "COD_CTA_REF"] = novos_dados[0]
                    df_res.loc[mask_alvo, "ORIGEM_MAP"] = novos_dados[1]

        # Preenche ORIGEM_MAP para os que já vieram declarados via I051
        if "ORIGEM_MAP" not in df_res.columns:
            df_res["ORIGEM_MAP"] = "SEM_MAPEAMENTO"

        mask_declarado = cast(pd.Series, df_res["COD_CTA_REF"]).notna() & (
            cast(pd.Series, df_res["ORIGEM_MAP"]) == "SEM_MAPEAMENTO"
        )
        df_res.loc[mask_declarado, "ORIGEM_MAP"] = "I051"

        # Limpeza final: ORIGEM_MAP deve ser vazio para contas SINTÉTICAS
        mask_sintetica = df_res["IND_CTA"].astype(str).str.upper() != "A"
        df_res.loc[mask_sintetica, "ORIGEM_MAP"] = ""

        if "CTA" in df_res.columns:
            df_res["CONTA"] = (
                cast(pd.Series, df_res["COD_CTA"]).astype(str)
                + " - "
                + cast(pd.Series, df_res["CTA"]).astype(str).str.strip().str.upper()
            )
        return cast(pd.DataFrame, df_res)

    def processar_lancamentos(self, df_plano: pd.DataFrame) -> pd.DataFrame:
        """Processa Lançamentos Contábeis (I200/I250)."""
        df_i200 = self.blocos.get("dfECD_I200")
        df_i250 = self.blocos.get("dfECD_I250")
        if df_i200 is None or df_i250 is None:
            return pd.DataFrame()

        df_lctos = pd.merge(
            df_i200[["PK", "NUM_LCTO", "DT_LCTO", "IND_LCTO"]],
            df_i250,
            left_on="PK",
            right_on="FK_PAI",
        )

        df_lctos["CNPJ"] = self.cnpj
        df_lctos["VL_D"] = df_lctos.apply(
            lambda r: self._converter_decimal(r.get("VL_DC"))
            if r.get("IND_DC") == "D"
            else Decimal("0.00"),
            axis=1,
        )
        df_lctos["VL_C"] = df_lctos.apply(
            lambda r: self._converter_decimal(r.get("VL_DC"))
            if r.get("IND_DC") == "C"
            else Decimal("0.00"),
            axis=1,
        )
        df_lctos["VL_SINAL"] = df_lctos.apply(
            lambda r: Decimal(str(r.get("VL_D", "0")))
            - Decimal(str(r.get("VL_C", "0"))),
            axis=1,
        )

        if not df_plano.empty:
            df_lctos = pd.merge(
                df_lctos, df_plano[["COD_CTA", "CONTA"]], on="COD_CTA", how="left"
            )

        return df_lctos

    def gerar_balancetes(self) -> Dict[str, pd.DataFrame]:
        """
        Gera balancetes com Forward Roll, Reversão de Encerramento.
        """
        df_plano = self.processar_plano_contas()
        df_i150 = self.blocos.get("dfECD_I150")
        df_i155 = self.blocos.get("dfECD_I155")
        df_i157 = self.blocos.get("dfECD_I157")  # Transferência de Plano de Contas

        if df_plano.empty or df_i150 is None or df_i155 is None:
            return {}

        # 1. Base Unificada de Saldos
        df_base = pd.merge(
            df_i150[["PK", "DT_FIN"]], df_i155, left_on="PK", right_on="FK_PAI"
        )
        df_base["CNPJ"] = self.cnpj

        # 2. Sinais e Tipagem
        df_base["VL_SLD_INI_SIG"] = df_base.apply(
            lambda r: self._converter_decimal(r.get("VL_SLD_INI"))
            if r.get("IND_DC_INI") == "D"
            else -self._converter_decimal(r.get("VL_SLD_INI")),
            axis=1,
        )
        df_base["VL_SLD_FIN_SIG"] = df_base.apply(
            lambda r: self._converter_decimal(r.get("VL_SLD_FIN"))
            if r.get("IND_DC_FIN") == "D"
            else -self._converter_decimal(r.get("VL_SLD_FIN")),
            axis=1,
        )
        df_base["VL_DEB"] = cast(pd.Series, df_base["VL_DEB"]).apply(
            self._converter_decimal
        )
        df_base["VL_CRED"] = cast(pd.Series, df_base["VL_CRED"]).apply(
            self._converter_decimal
        )

        # 3. Reversão de Encerramento (Indicator 'E')
        df_lctos = self.processar_lancamentos(df_plano)
        if not df_lctos.empty and "IND_LCTO" in df_lctos.columns:
            df_e = df_lctos[df_lctos["IND_LCTO"] == "E"].copy()
            if not df_e.empty:
                ajustes = (
                    df_e.groupby(["COD_CTA", "DT_LCTO"])
                    .agg({"VL_SINAL": "sum", "VL_D": "sum", "VL_C": "sum"})
                    .reset_index()
                )
                ajustes.rename(
                    columns={
                        "VL_SINAL": "VL_AJ_SINAL",
                        "VL_D": "VL_AJ_D",
                        "VL_C": "VL_AJ_C",
                        "DT_LCTO": "DT_FIN",
                    },
                    inplace=True,
                )

                df_base = pd.merge(
                    df_base, ajustes, on=["COD_CTA", "DT_FIN"], how="left"
                ).fillna(Decimal("0.00"))
                df_base["VL_SLD_FIN_SIG"] = cast(
                    pd.Series, df_base["VL_SLD_FIN_SIG"]
                ) - cast(pd.Series, df_base["VL_AJ_SINAL"])
                df_base["VL_DEB"] = cast(pd.Series, df_base["VL_DEB"]) - cast(
                    pd.Series, df_base["VL_AJ_D"]
                )
                df_base["VL_CRED"] = cast(pd.Series, df_base["VL_CRED"]) - cast(
                    pd.Series, df_base["VL_AJ_C"]
                )

        # 4. Forward Roll (Continuidade Histórica) & I157
        df_base = df_base.sort_values(["COD_CTA", "DT_FIN"])
        df_base["VL_SLD_FIN_ANT"] = df_base.groupby("COD_CTA")["VL_SLD_FIN_SIG"].shift(
            1
        )

        # Se houver I157, aplica o mapeamento de saldos iniciais transferidos
        if df_i157 is not None:
            df_base = pd.merge(
                df_base,
                df_i157[["COD_CTA", "VL_SLD_INI", "IND_DC_INI"]],
                on="COD_CTA",
                how="left",
                suffixes=("", "_I157"),
            )
            df_base["VL_I157_SIG"] = df_base.apply(
                lambda r: self._converter_decimal(r.get("VL_SLD_INI_I157"))
                if r.get("IND_DC_INI_I157") == "D"
                else -self._converter_decimal(r.get("VL_SLD_INI_I157")),
                axis=1,
            )
            # Aplica o saldo do I157 apenas se não houver saldo anterior detectado (início da conta no novo plano)
            mask_primeiro_mes = cast(pd.Series, df_base["VL_SLD_FIN_ANT"]).isna()
            df_base.loc[
                mask_primeiro_mes & cast(pd.Series, df_base["VL_I157_SIG"]).notna(),
                "VL_SLD_INI_SIG",
            ] = df_base["VL_I157_SIG"]

        df_base["VL_SLD_INI_SIG"] = df_base.apply(
            lambda r: r.get("VL_SLD_FIN_ANT")
            if pd.notna(r.get("VL_SLD_FIN_ANT"))
            else r.get("VL_SLD_INI_SIG"),
            axis=1,
        )

        # 5. Propagação Hierárquica (Plano da Empresa)
        balancete_empresa = self._propagar_hierarquia(df_base, df_plano)

        # 6. Balancete Referencial (baseRFB)
        balancete_rfb = self.gerar_balancete_referencial(df_base)

        return {
            "04_Balancetes_Mensais": balancete_empresa,
            "04_Balancetes_RFB": balancete_rfb,
        }

    def gerar_balancete_referencial(self, df_saldos: pd.DataFrame) -> pd.DataFrame:
        """
        Gera o balancete na visão do Plano Referencial da RFB.
        """
        # 1. Localiza e unifica todos os blocos do Plano Referencial (Balanço + Resultado)
        caminhos = self._obter_arquivos_referenciais()
        if not caminhos:
            logger.warning(
                "Nenhum arquivo de plano referencial localizado no catálogo."
            )
            return pd.DataFrame()

        dfs_schemas = []
        for p in caminhos:
            try:
                dfs_schemas.append(pd.read_csv(p, sep="|", dtype=str))
            except Exception as e:
                logger.error(f"Erro ao carregar CSV referencial {p}: {e}")

        if not dfs_schemas:
            return pd.DataFrame()

        df_ref_schema = pd.concat(dfs_schemas, ignore_index=True)

        # 2. Prepara os saldos analíticos da empresa mapeados para o referencial
        df_plano = self.processar_plano_contas()
        if "COD_CTA_REF" not in df_plano.columns:
            return pd.DataFrame()

        # Join dos saldos com o mapeamento referencial
        cols_valores = ["VL_SLD_INI_SIG", "VL_DEB", "VL_CRED", "VL_SLD_FIN_SIG"]
        df_mapeado = pd.merge(
            df_saldos[cols_valores + ["COD_CTA", "DT_FIN"]],
            df_plano[["COD_CTA", "COD_CTA_REF"]],
            on="COD_CTA",
            how="inner",
        )

        # Filtra apenas registros que possuem mapeamento referencial
        df_mapeado = df_mapeado[
            cast(pd.Series, df_mapeado["COD_CTA_REF"]).notna()
            & (cast(pd.Series, df_mapeado["COD_CTA_REF"]) != "")
        ]

        if df_mapeado.empty:
            return pd.DataFrame()

        # Agrupa por Conta Referencial e Data (pois várias contas da empresa podem mapear p/ uma referencial)
        df_analitico_ref = (
            df_mapeado.groupby(["COD_CTA_REF", "DT_FIN"])[cols_valores]
            .sum()
            .reset_index()
        )

        # 3. Consolidação Hierárquica no Plano Referencial
        balancetes_rfb = []
        versoes_data = cast(pd.Series, df_analitico_ref["DT_FIN"]).unique()
        for data in versoes_data:
            df_mes = df_analitico_ref[df_analitico_ref["DT_FIN"] == data].copy()

            # Prepara a tabela base do mês com TODAS as contas do plano referencial
            tab = df_ref_schema.copy()
            tab = pd.merge(
                tab,
                cast(pd.DataFrame, df_mes),
                left_on="CODIGO",
                right_on="COD_CTA_REF",
                how="left",
            )

            # Converte valores p/ Decimal
            for col in cols_valores:
                tab[col] = cast(pd.Series, tab[col]).apply(self._converter_decimal)

            # Algoritmo Bottom-Up no Plano Referencial
            tab["NIVEL"] = (
                cast(
                    pd.Series,
                    pd.to_numeric(cast(pd.Series, tab["NIVEL"]), errors="coerce"),
                )
                .fillna(0)
                .astype(int)
            )
            niveis = sorted(cast(pd.Series, tab["NIVEL"]).unique(), reverse=True)

            for nivel in niveis:
                if nivel <= 1:
                    continue

                agregados = (
                    tab[tab["NIVEL"] == nivel]
                    .groupby("COD_SUP")[cols_valores]
                    .sum()
                    .reset_index()
                )

                for _, row in agregados.iterrows():
                    idx_pai = tab.index[tab["CODIGO"] == row["COD_SUP"]]
                    if not idx_pai.empty:
                        tab.loc[idx_pai, cols_valores] += row[cols_valores]

            tab["DT_FIN"] = data
            if "COD_CTA_REF" in tab.columns:
                tab.drop(columns=["COD_CTA_REF"], inplace=True)
            balancetes_rfb.append(tab)

        return (
            cast(pd.DataFrame, pd.concat(balancetes_rfb, ignore_index=True))
            if balancetes_rfb
            else pd.DataFrame()
        )

    def _propagar_hierarquia(
        self, df_saldos: pd.DataFrame, df_plano: pd.DataFrame
    ) -> pd.DataFrame:
        """Algoritmo Bottom-Up para consolidação de níveis sintéticos."""
        balancetes = []
        cols_valores = ["VL_SLD_INI_SIG", "VL_DEB", "VL_CRED", "VL_SLD_FIN_SIG"]

        versoes_data = cast(pd.Series, df_saldos["DT_FIN"]).unique()
        for data in versoes_data:
            df_mes = df_saldos[df_saldos["DT_FIN"] == data].copy()
            tab = pd.merge(
                df_plano,
                cast(pd.DataFrame, df_mes)[cols_valores + ["COD_CTA"]],
                on="COD_CTA",
                how="left",
            )
            for col in cols_valores:
                tab[col] = cast(pd.Series, tab[col]).apply(self._converter_decimal)

            niveis = sorted(cast(pd.Series, tab["NIVEL"]).unique(), reverse=True)
            for nivel in niveis:
                if nivel == 1:
                    continue
                agregados = (
                    tab[tab["NIVEL"] == nivel]
                    .groupby("COD_CTA_SUP")[cols_valores]
                    .sum()
                    .reset_index()
                )
                for _, row in agregados.iterrows():
                    idx_pai = tab.index[tab["COD_CTA"] == row["COD_CTA_SUP"]]
                    if not idx_pai.empty:
                        tab.loc[idx_pai, cols_valores] += row[cols_valores]

            tab["DT_FIN"] = data
            balancetes.append(tab)

        return (
            pd.concat(balancetes, ignore_index=True) if balancetes else pd.DataFrame()
        )

    def processar_demonstracoes(self) -> Dict[str, pd.DataFrame]:
        """Processa Balanço (J100) e DRE (J150)."""
        df_j100 = self.blocos.get("dfECD_J100")
        df_j150 = self.blocos.get("dfECD_J150")
        df_j005 = self.blocos.get("dfECD_J005")

        res = {"BP": pd.DataFrame(), "DRE": pd.DataFrame()}
        if df_j005 is not None:
            # Garante que temos a data final para o join
            base = (
                df_j005[["PK", "DT_FIN", "CNPJ"]]
                if "CNPJ" in df_j005.columns
                else df_j005[["PK", "DT_FIN"]]
            )
            if df_j100 is not None:
                res["BP"] = pd.merge(base, df_j100, left_on="PK", right_on="FK_PAI")
            if df_j150 is not None:
                res["DRE"] = pd.merge(base, df_j150, left_on="PK", right_on="FK_PAI")
        return res
